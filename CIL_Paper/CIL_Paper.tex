\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm2e}
\usepackage{pgfplots}
\pgfplotsset{compat=1.10}

\begin{document}
\title{Evaluating the Augmented Lagrangian Method for matrix completion used for Collaborative Filtering}

\author{
  Sven Hammann, Stefan Dietiker, Jannick Griner\\
  Department of Computer Science, ETH Zurich, Switzerland
}



\maketitle

\begin{abstract}
We evaluate the performance of a collaborative filtering algorithm that uses the
Augmented Lagrangian Method (ALM) to minimize the nuclear norm of the reconstructed matrix.
We present a slightly simpler version of this algorithm and show that is still performs
well on different data sets, including data that contains corrupted data from malicious attackers.
In particular, we show that the algorithm performs better than robust principal component analysis
on these data sets, which is specifically used to filter out corrupted entries. We also compare the
ALM algorithm to a baseline SVD algorithm and show that it performs better in terms of mean squared
error, but has a longer runtime.
\end{abstract}

\section{Introduction}
Recommender systems are widely used in practice to find items, e.g. books, movies or music,
that might be of interest to a user. This goal can be achieved through the application of collaborative filtering algorithms, which try to predict ratings of
users on items they have not actually rated. They do so by building a model based on the known ratings of a user and ratings of other, similar users. \\

In our model, we consider an $m \times n$ matrix consisting of the ratings of m users for n items,
where many values are missing. The goal of the collaborative filtering
algorithm is to predict the missing values as accurately as possible. Its performance is measured
by comparing a full original data matrix with the matrix that was reconstructed by the algorithm,
given the original matrix with a fraction of entries missing. \\

The main assumption of many collaborative filtering algorithms, including the ones used in this paper,
is that the original matrix has low rank, or at least approximately low rank, i.e., many dimensions
only having small singular values. The reason for this lies in the assumed correlations between ratings:
If two or more users have similar ratings on some items, it is likely that there is some underlying smaller
dimension responsible for these ratings. These dimensions can be interpreted as topics of interest. \\

A natural procedure is now to find a solution that respects the known entries, i.e., the reconstruction is equal 
to the original on the entries that are known, and that has minimal rank. A good approximation for minimization
of the rank is minimizing the nuclear norm, i.e., the sum of singular values. This can be solved by convex optimization. 
The algorithm we use was presented in \cite{almpaper} and uses the Augmented Lagrangian Method (ALM). \\

An additional consideration is that of corrupted entries in the original data matrix. These values are not missing,
but they may not reflect a true rating of a user. An important real-world example are shilling attacks, where companies
upvote their own products and downgrade the products of competitors. In a scenario where such attacks are expected, it
is advisable to use a different method that takes care of these corruptions. We compare the algorithm using ALM with another algorithm that performs Robust Principal Component Analysis (RPCA), as introduced in \cite{rpcapaper}, and can deal with corrupted entries. \\

We compare the algorithm using ALM from \cite{almpaper} to an algorithm using RPCA (as in \cite{rpcapaper}) and to another baseline algorithm using one Singular Value Decomposition. We present our own model to generate artificial test data that simulates users rating hotels that belong to different groups, and attackers that may want to benefit one group and hurt the others. We test our algorithms on our own artificial data as well as on a different data set. We then choose the ALM algorithm based on its performance on these data sets.

\section{Models and Methods}

\subsection{Related Work}

The algorithm we use is based on Algorithm 6 in \cite{almpaper}.
We use a simplified version of it that does not include the update step
for $\mu$ and uses the economy-size singular value decomposition directly
provided by MATLAB rather than an external library. \\

We compare these results to those of another algorithm based on \cite{rpcapaper},
Section 1.6, that explains how Robust PCA can be used for matrix completion. \\

\subsection{Objective}

We formulate our problem as in Section 3.2 of \cite{almpaper}. Given an original matrix $D$ and
a number of observed entries $\Omega$, we wish to reconstruct the complete matrix $D$. Under certain
conditions on the rank of $D$ and the number of samples, $D$ can be recovered exactly \cite{exactpaper}. 
In the case of collaborative filtering, we may be given a matrix $D$ that is only
approximately low-rank, i.e., it has many dimensions with small singular values. In this case, a good approximation
is still possible. \\

Let $A$ be our matrix with the objective to reconstruct the original $D$. We cannot use convex optimization to minimize $rank(A)$ directly, and we might not want to do this if $A$ has many small, but nonzero singular values. We instead minimize the nuclear norm of $A$, denoted by $||A||_*$, which is the sum of singular values of $A$. This provides a good approximation for minimizing $rank(A)$. This is proven with respect to Robust PCA in \cite{rpcapaper} and adapted for our setting in \cite{almpaper}. \\

We want so solve the following problem: 
$$ \underset{A}{\text{minimize }} ||A||_* \text{ subject to  } A_{ij} = D_{ij} \ \forall(i, j) \in \Omega$$

Let $D_{\Omega}$ be the matrix such that $(D_{\Omega})_{ij} = D_{ij} \ \forall(i, j) \in \Omega$, and $(D_{\Omega})_{ij} = 0 \ \forall(i, j) \notin \Omega$. That is, the missing values are set to zero. We can then rewrite the constraint by introducing an auxiliary variable matrix $E$:

$$ \underset{A}{\text{minimize }} ||A||_* \text{ subject to  }$$ 
$$A + E = D_{\Omega} \ \ \ E_{ij} = 0 \ \forall(i, j) \in \Omega$$

Since $E$ may be nonzero only on the coordinates of missing values, its only purpose is to compensate for the values set in $A$ on these coordinates. As $D_{\Omega}$ is zero there, if $A_{ij} = x$, we must have $D_{ij} = -x$ to fulfill the constraint. \\

\subsection{Algorithm}

We use a simplified version of Algorithm 6 in \cite{almpaper}, ALM for matrix completion. We introduce the soft-thresholding operator used in \cite{almpaper}: \\

$$\mathcal{S}_{\epsilon}[x] := sign(x) \cdot max(abs(x) - \epsilon, 0)$$

It lets $x$ go by $\epsilon$ closer to zero. The operator can be used on a matrix by applying it element-wise.


\begin{algorithm}
\KwIn{Observation samples $D_{ij}$, $(i, j) \in \Omega$, of a matrix $D \in \mathbb{R}^{m \times n}$ }
$Y = 0$; $E = 0$; $\mu > 0$; $iter = 0$ \\
\ \\

\While{$\frac{||D_{\Omega} - A - E||_F}{||D_{\Omega}||_F} > \epsilon_1$ AND $iter < maxIter$}{
$(U, S, V) = svd(D_{\Omega} - E + \mu^{-1}Y)$; \\
$A = U\mathcal{S}_{\mu^-1}[S]V^T$; \\
$E_{\bar{\Omega}} = (-A + \mu^{-1}Y)_{\bar{\Omega}}$; \\
$Y = Y + \mu(D_{\Omega} - A - E)$; \\
$iter = iter + 1$;
}
\KwOut{$A$}
\end{algorithm}

We refer to Algorithm 6 of \cite{almpaper} for the detailed workings on the algorithm. The main difference is
that we do not update $\mu$; in our testings, we found this to be unneccessary for good results, and chose
the simpler version to work with.

\subsection{Implementation details}

We use MATLAB for the implementation of our algorithm, and do not use any extra software packages.
For $svd$, we use the economy-size singular value decomposition provided by MATLAB. \\

We set $\mu = ||D_{\Omega}||_2^{-1}$, which is used in \cite{almpaper} as starting value $\mu_0$ that later gets updated. \\

Our convergence criterion $\epsilon_1$ and the maximum number of iterations $maxIter$ are both variables that may be adjusted
to get a trade-off between runtime and accuracy. As shown in the result section, increasing $\epsilon_1$ and/or decreasing $maxIter$
may result in a better runtime with only little loss in accuracy. We do not give an optimal choice for these values and refer to the Results section for trade-off considerations.


\subsection{Data generation model}

We created a model to generate artificial data simulating users that rate hotels on a scale from 0 to 100. The model comes up with data in several steps. First, users and hotels are randomly divided into groups. In the next step, a coupling matrix, representing the similarity between the hotel groups is introduced. Based on this group similarity matrix and some randomness, a matrix representing the rating of hotel groups by user groups is created. Now we have a representation of the assumed underlying model of the rating process. User-hotel ratings can now be generated based on their respective groups. Again some extra randomness is added to represent the users individuality. Additionally the found rating is biased towards an underlying global score of the hotel, representing it's "true" rating.\\
Attacks are added after the proper rating matrix has been created. A certain number of randomly selected hotels "attack" the system. The attackers are split into groups. Attack rows are added with the following ratings $R(h)$ for hotels:
\begin{equation*}
R(h) =
\left\{
	\begin{array}{ll}
		\mathcal{N}(98,2)  & \mbox{if } h \in \mbox{Attacker's group} \\
		\mathcal{N}(2,2) & \mbox{if } h \notin \mbox{Attacker's group}
	\end{array}
\right.
\label{eq:}
\end{equation*}

The attacker rows are added to the end of the data matrix.


\section{Results}

\begin{tikzpicture}
\begin{axis}[
	legend pos=north west,
	title=Runtime Plot,
	xlabel={Percentage of data considered},
	ylabel={Runtime in $s$},
	grid=major,
	legend entries={$SVD$,$ALM$,$RPCA$},
]
\addplot table {data/runtime_svd.dat};
\addplot table {data/runtime_alm.dat};
\addplot table {data/runtime_rpca.dat};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
	legend pos=north west,
	title=Sparsity Plot (with added attackers),
	xlabel={Sparsity of Matrix},
	ylabel={Mean error},
	grid=major,
	legend entries={$ALM$,$RPCA$,$SVD$},
]
\addplot [mark=square*,blue,solid] table[x=Sparse,y=alm] {data/Sparse/sparsity_0.dat};
\addplot [mark=square*,green,solid]	table[x=Sparse,y=rpca] {data/Sparse/sparsity_0.dat};
\addplot [mark=square*,red,solid] table[x=Sparse,y=svd] {data/Sparse/sparsity_0.dat};
\addplot [mark=o,blue,dotted] table[x=Sparse,y=alm] {data/Sparse/sparsity_100.dat};
\addplot [mark=o,green,dotted] table[x=Sparse,y=rpca] {data/Sparse/sparsity_100.dat};
\addplot [mark=o,red,dotted] table[x=Sparse,y=svd] {data/Sparse/sparsity_100.dat};
\addplot [mark=o,blue,dotted] table[x=Sparse,y=alm] {data/Sparse/sparsity_200.dat};
\addplot [mark=o,green,dotted] table[x=Sparse,y=rpca] {data/Sparse/sparsity_200.dat};
\addplot [mark=o,red,dotted] table[x=Sparse,y=svd] {data/Sparse/sparsity_200.dat};
\addplot [mark=o,blue,dotted] table[x=Sparse,y=alm] {data/Sparse/sparsity_300.dat};
\addplot [mark=o,green,dotted] table[x=Sparse,y=rpca] {data/Sparse/sparsity_300.dat};
\addplot [mark=o,red,dotted] table[x=Sparse,y=svd] {data/Sparse/sparsity_300.dat};
\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
\begin{axis}[
	legend pos=outer north east,
	title=User Group Plot,
	xlabel={Number of user groups},
	ylabel={Mean error},
	grid=major,
	legend entries={$ALM$,$RPCA$,$SVD$},
]
\addplot table[x=groups,y=alm] {data/userGroup/realGroups.dat};
\addplot table[x=groups,y=rpca] {data/userGroup/realGroups.dat};
\addplot table[x=groups,y=svd] {data/userGroup/realGroups.dat};
\end{axis}
\end{tikzpicture}


\section{Discussion}



\section{Summary}


\bibliographystyle{IEEEtran}
\bibliography{CIL_Paper}
\end{document}
