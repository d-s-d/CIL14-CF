\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm2e}


\begin{document}
\title{Evaluating the Augmented Lagrangian Method for matrix completion used for Collaborative Filtering}

\author{
  Sven Hammann, Stefan Dietiker, Jannick Griner\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
We evaluate the performance of a collaborative filtering algorithm that uses the
Augmented Lagrangian Method (ALM) to minimize the nuclear norm of the reconstructed matrix.
We present a slightly simpler version of this algorithm and show that is still performs
well on different data sets, including data that contains corrupted data from malicious attackers.
In particular, we show that the algorithm performs better than robust principal component analysis
on these data sets, which is specifically used to filter out corrupted entries. We also compare the
ALM algorithm to a baseline SVD algorithm and show that it performs better in terms of mean squared
error, but has a longer runtime.
\end{abstract}

\section{Introduction}
Recommender systems are widely used in practice to present items, e.g. books, movies or music,
that are of interest to the user. Collaborative filtering algorithms try to predict ratings of
users on items they have not yet rated. They do so by relating the ratings of the user for other
items to those of other users. \\

In our model, we consider an $m \times n$ matrix that consists of the ratings of m users for n items,
where many values are missing, denoted by a special value. The goal of the collaborative filtering
algorithm is to predict the missing values as accurately as possible. Its performance is measured
by comparing a full original data matrix with the matrix that was reconstructed by the algorithm,
given a part of the original matrix with missing entries. \\

The main assumption of many collaborative filtering algorithms including the one used in this paper
is that the original matrix has low rank, or at least approximately low rank, i.e., many dimensions
that have small singular values. The reason for this lies in the assumed correlations between ratings:
If two or more users have similar ratings on some items, it is likely that there is some underlying smaller
dimension responsible for these ratings. These dimensions can be interpreted as topics of interest. \\

A natural procedure is now to find a solution that respects the known entries, i.e., the reconstruction is equal 
to the original on the entries that are known, and that has minimal rank. A good approximation for minimization
of the rank is minimizing the nuclear norm, i.e., the sum of singular values. This can be solved by convex optimization. 
The algorithm we use was presented in \cite{almpaper} and uses the Augmented Lagrangian Method (ALM). \\

An additional consideration is that of corrupted entries in the original data matrix. These values are not missing,
but they may not reflect a true rating of a user. An important real-world example are shilling attacks, where companies
upvote their own products and downgrade the products of competitors. In a scenario where such attacks are expected, it
is advisable to use a different method that takes care of these corruptions. We compare the algorithm using ALM with another algorithm that performs Robust Principal Component Analysis (RPCA), as introduced in \cite{rpcapaper}, and can deal with corrupted entries. \\

We compare the algorithm using ALM from \cite{almpaper} to an algorithm using RPCA (as in \cite{rpcapaper}) and to another baseline algorithm using one Singular Value Decomposition. We present our own model to generate artificial test data that simulates users rating hotels that belong to different groups, and attackers that may want to benefit one group and hurt the others. We test our algorithms on our own artificial data as well as on a different data set. We then choose the ALM algorithm based on its performance on these data sets.

\section{Models and Methods}

\subsection{Related Work}

The algorithm we use is based on Algorithm 6 in \cite{almpaper}.
We use a simplified version of it that does not include the update step
for $\mu$ and uses the economy-size singular value decomposition directly
provided by MATLAB rather than an external library. \\

We compare these results to those of another algorithm based on \cite{rpcapaper},
Section 1.6, that explains how Robust PCA can be used for matrix completion. \\

\subsection{Objective}

We formulate our problem as in Section 3.2 of \cite{almpaper}. Given an original matrix $D$ and
a number of observed entries $\Omega$, we wish to reconstruct the complete matrix $D$. Under certain
conditions on the rank of $D$ and the number of samples, $D$ can be recovered exactly \cite{exactpaper}. 
In the case of collaborative filtering, we may be given a matrix $D$ that is only
approximately low-rank, i.e., it has many dimensions with small singular values. In this case, a good approximation
is still possible. \\

We want so solve the following problem: 
$$ \underset{A}{\text{minimize }} ||A||_* \text{ subject to  } A_{ij} = D_{ij} \ \forall(i, j) \in \Omega$$

Let $D_{\Omega}$ be the matrix such that $(D_{\Omega})_{ij} = D_{ij} \ \forall(i, j) \in \Omega$, and $(D_{\Omega})_{ij} = 0 \ \forall(i, j) \notin \Omega$. That is, the missing values are set to zero. We can then rewrite the constraint by introducing an auxiliary variable matrix $E$:

$$ \underset{A}{\text{minimize }} ||A||_* \text{ subject to  }$$ 
$$A + E = D_{\Omega} \ \ \ E_{ij} = 0 \ \forall(i, j) \in \Omega$$

Since $E$ may be nonzero only on the coordinates of missing values, its only purpose is to compensate for the values set in $A$ on these coordinates. As $D_{\Omega}$ is zero there, if $A_{ij} = x$, we must have $D_{ij} = -x$ to fulfill the constraint. \\

\subsection{Algorithm}

We use a simplified version of Algorithm 6 in \cite{almpaper}, ALM for matrix completion. We introduce the soft-thresholding operator used in \cite{almpaper}: \\

$$\mathcal{S}_{\epsilon}[x] := sign(x) \cdot max(abs(x) - \epsilon, 0)$$

It lets $x$ go by $\epsilon$ closer to zero. The operator can be used on a matrix by applying it element-wise.


\begin{algorithm}
\KwIn{Observation samples $D_{ij}$, $(i, j) \in \Omega$, of a matrix $D \in \mathbb{R}^{m \times n}$ }
$Y = 0$; $E = 0$; $\mu > 0$; $iter = 0$ \\
\ \\

\While{$\frac{||D_{\Omega} - A - E||_F}{||D_{\Omega}||_F} > \epsilon_1$ AND $iter < maxIter$}{
$(U, S, V) = svd(D_{\Omega} - E + \mu^{-1}Y)$; \\
$A = U\mathcal{S}_{\mu^-1}[S]V^T$; \\
$E_{\bar{\Omega}} = (-A + \mu^{-1}Y)_{\bar{\Omega}}$; \\
$Y = Y + \mu(D_{\Omega} - A - E)$; \\
$iter = iter + 1$;
}
\KwOut{$A$}
\end{algorithm}

We refer to Algorithm 6 of \cite{almpaper} for the detailed workings on the algorithm. The main difference is
that we do not update $\mu$; in our testings, we found this to be unneccessary for good results, and chose
the simpler version to work with.

\subsection{Implementation details}

We use MATLAB for the implementation of our algorithm, and do not use any extra software packages.
For $svd$, we use the economy-size singular value decomposition provided by MATLAB. \\

We set $\mu = ||D_{\Omega}||_2^{-1}$, which is used in \cite{almpaper} as starting value $\mu_0$ that later gets updated. \\

Our convergence criterion $\epsilon_1$ and the maximum number of iterations $maxIter$ are both variables that may be adjusted
to get a trade-off between runtime and accuracy. As shown in the result section, increasing $\epsilon_1$ and/or decreasing $maxIter$
may result in a better runtime with only little loss in accuracy. We do not give an optimal choice for these values and refer to the Results section for trade-off considerations.


\subsection{Data generation model}

We used a model to generate artificial data simulating users that rate hotels on a scale from 0 to 100. [TODO details of hotel model]






\section{Results}


\section{Discussion}



\section{Summary}


\bibliographystyle{IEEEtran}
\bibliography{CIL_Paper}
\end{document}
